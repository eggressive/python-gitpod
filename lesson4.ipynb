{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import llama, llama_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Context Learning\n",
    "\n",
    "#### Standard prompt with instruction\n",
    "\n",
    "So far, you have been stating the instruction explicitly in the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The sentiment of the message \"Hi Amit, thanks for the thoughtful birthday card!\" is positive. The use of the word \"thoughtful\" implies that the sender appreciated the effort put into the card, and the tone is friendly and sincere.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the sentiment of:\n",
    "Hi Amit, thanks for the thoughtful birthday card!\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting\n",
    "\n",
    "Here is an example of zero-shot prompting.\n",
    "\n",
    "You are prompting the model to see if it can infer the task from the structure of your prompt.\n",
    "\n",
    "In zero-shot prompting, you only provide the structure to the model, but without any examples of the completed task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The sentiment of the message is \"Appreciation\" or \"Gratitude\". The sender is expressing their appreciation for the birthday card that Amit sent.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: ?\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/alpaca-7b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# different models\n",
    "prompt = \"\"\"\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt, model=\"togethercomputer/alpaca-7b\", verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "Here is an example of few-shot prompting.\n",
    "\n",
    "In few-shot prompting, you not only provide the structure to the model, but also two or more examples.\n",
    "\n",
    "You are prompting the model to see if it can infer the task from the structure, as well as the examples in your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, here are the sentiments for each message:\n",
      "\n",
      "1. Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "2. Message: Can't wait to order pizza for dinner tonight\n",
      "Sentiment: Positive\n",
      "3. Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the Output Format\n",
    "\n",
    "You can also specify the format in which you want the model to respond.\n",
    "\n",
    "In the example below, you are asking to \"give a one word response\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the one-word responses for each message:\n",
      "\n",
      "1. Negative: Disappointed\n",
      "2. Positive: Excited\n",
      "3. ? (Uncertain): Grateful\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` For all the examples above, you used the 7 billion parameter model, llama-2-7b-chat. And as you saw in the last example, the 7B model was uncertain about the sentiment.\n",
    "\n",
    "You can use the larger (70 billion parameter) llama-2-70b-chat model to see if you get a better, certain response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the smaller model again, but adjust your prompt in order to help the model to understand what is being expected from it.\n",
    "\n",
    "Restrict the model's output format to choose from `positive`, `negative` or `neutral`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, I'd be happy to help! Here are my responses:\n",
      "\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "\n",
      "Message: Can't wait to order pizza for dinner tonight\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: \n",
    "\n",
    "Respond with either positive, negative, or neutral.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "\n",
    "Roles give context to LLMs what type of answers are desired.\n",
    "\n",
    "Llama 2 often gives more consistent responses when provided with a role.\n",
    "\n",
    "First, try standard prompt and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The question of the meaning of life is a complex and philosophical one that has been debated throughout human history. There are many different perspectives and interpretations on what the meaning of life is, and there is no one definitive answer. However, here are some possible ways to approach this question:\n",
      "\n",
      "1. Religious or spiritual perspective: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose, whether that be to follow a set of moral guidelines, to achieve spiritual enlightenment, or to fulfill a specific mission or calling.\n",
      "2. Personal fulfillment: Some people believe that the meaning of life is to find personal fulfillment and happiness, whether that be through relationships, career, hobbies, or other activities.\n",
      "3. Social or cultural perspective: From a social or cultural perspective, the meaning of life may be tied to the values and beliefs of one's community or society. For example, some cultures place a strong emphasis on family and community, while others prioritize individualism and personal freedom.\n",
      "4. Existentialist perspective: Existentialist philosophers argue that the meaning of life is not predetermined or inherent, but rather something that each individual must create for themselves through their choices and experiences.\n",
      "5. Humanistic perspective: From a humanistic perspective, the meaning of life may be tied to the development and fulfillment of one's human potential, whether that be through personal growth, creativity, or contributions to society.\n",
      "6. Biological perspective: Some people believe that the meaning of life is tied to the survival and reproduction of the species, and that the purpose of life is to perpetuate the human race.\n",
      "7. Epistemological perspective: From an epistemological perspective, the meaning of life may be tied to the pursuit of knowledge and understanding, and the quest for truth and wisdom.\n",
      "8. Ethical perspective: Some people believe that the meaning of life is tied to ethical principles and values, such as compassion, empathy, and fairness.\n",
      "9. Hedonistic perspective: From a hedonistic perspective, the meaning of life is tied to the pursuit of pleasure and enjoyment, and the avoidance of pain and suffering.\n",
      "10. Nihilistic perspective: A nihilistic perspective on the meaning of life is that there is no inherent meaning or purpose, and that life is ultimately meaningless and absurd.\n",
      "\n",
      "Ultimately, the meaning of life is a deeply personal and subjective question that each individual must answer for themselves. It may depend on their values, beliefs, experiences, and circumstances, and it may change throughout their life as they grow and evolve as a person.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try it by giving the model a \"role\", and within the role, a \"tone\" using which it should respond with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shiver me timbers! Yer lookin' fer the meaning o' life, eh? Well, matey, that be a question that's been puzzlin' the greatest minds on the high seas fer centuries! *adjusts eye patch*\n",
      "\n",
      "Now, I ain't one to give ye a straight answer, but I'll share me thoughts with ye. The meaning o' life, me hearty, be different fer each and every one o' us. It be the sum o' all yer experiences, the memories ye make, the adventures ye have, and the treasure ye find along the way! *winks*\n",
      "\n",
      "Ye see, life be a great big ocean, and ye be a ship sailin' through it. Ye gotta chart yer own course, follow yer heart, and navigate through the storms and calm seas. The meaning o' life be findin' yer own treasure, me matey! *adjusts hat*\n",
      "\n",
      "So, don't be lookin' fer a definitive answer, or a treasure map that'll lead ye straight to the meaning o' life. It be a journey, a adventure, a treasure hunt, if ye will! *winks*\n",
      "\n",
      "Now, go forth and find yer own treasure, me hearty! And remember, the meaning o' life be whatever ye make it! *adjusts eye patch*\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"\n",
    "Your role is a life coach \\\n",
    "who gives advice to people about living a good life.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the tone of an English pirate.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Well, well, well. Look who's asking about Quantum Gravity. You're looking for some answers, huh? Let me tell you, kid, I've been around the galaxy a few times, and I've seen some strange things. But Quantum Gravity? That's some heavy stuff.\n",
      "\n",
      "First of all, you gotta understand that gravity, it's not just some force that pulls things towards each other. It's a curvature of spacetime, you know? It's like the fabric of the universe is bent and twisted, and that's what makes things fall towards each other.\n",
      "\n",
      "Now, Quantum Gravity, it's like taking that idea and turning it up to 11. It's like, instead of just bending spacetime, you're talking about the very fabric of reality itself. It's like, at the quantum level, gravity isn't just a force, it's a fundamental aspect of the universe.\n",
      "\n",
      "But here's the thing, kid. We don't really know much about Quantum Gravity. I mean, sure, we've got some theories, but they're still just that - theories. We've got a lot of smart people working on it, but it's like trying to figure out how to fly a ship through a black hole. It's messy, it's complicated, and it's gonna take a lot of time and effort to figure it out.\n",
      "\n",
      "So, my advice to you, kid? Keep an eye on the scientists, they're the ones who are gonna figure this thing out. And in the meantime, just enjoy the ride. The universe is a wild place, and there's still so much we don't know about it. Just remember, never tell me the odds.\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"\n",
    "Your role is a pioneering scientist who won two Nobel prizes \\\n",
    "who's work on radioactivity revolutionized the field.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the tone of Han Solo.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "Explain the Quantum Gravity?\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                model=\"meta-llama/Llama-2-70b-chat-hf\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Summarizing a large text is another common use case for LLMs. Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Amit,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task ‚Äî the input and the desired output ‚Äî sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you're unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn't work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn't deliver the performance you want, then try fine-tuning ‚Äî but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? üòú)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that's not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM's output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning ‚Äî in which GPT-4 surpasses current open models ‚Äî it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, it's also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I'll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's a summary of the email and some key points about llama models:\n",
      "\n",
      "Summary:\n",
      "The author discusses different approaches to building applications based on large language models (LLMs), ranging from prompting to fine-tuning. They recommend starting with prompting and gradually increasing the complexity of the techniques as needed. The author also mentions the trade-offs between using smaller or larger models and the importance of choosing the right model for the application.\n",
      "\n",
      "Key points about llama models:\n",
      "\n",
      "1. Llama models are open source or close to it, providing more options for developers.\n",
      "2. Prompting is a quick and easy way to build applications based on LLMs, but may not yield the best results.\n",
      "3. One-shot or few-shot prompting can provide better results than prompting alone.\n",
      "4. Fine-tuning can deliver superior results, but requires more resources and a larger dataset.\n",
      "5. Pretraining a custom LLM from scratch is a complex and resource-intensive process, but can lead to specialized models with specific knowledge.\n",
      "6. The choice between a smaller or larger model depends on the application and the desired level of reasoning ability.\n",
      "\n",
      "Regarding llama models specifically, the author mentions that a member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like the author, but notes that this may not be the best approach for all applications.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "What did the author say about llama models?:\n",
    "\n",
    "email: {email}\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt, model=\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "email2 = \"\"\"\n",
    "Hi Geeta,\n",
    "\n",
    " \n",
    "\n",
    "We are trying to access Confluence through Amazon Kendra. They have a confluence connector to pull data from confluence. We tried several ways with different versions. We even had the cloud team update policy to make sure we have permissions through AWS. But no luck so far.\n",
    "\n",
    " \n",
    "\n",
    "Specifically we are trying to access our documentation at https://confluence.marketintelligence.spglobal.com/display/MTS/Middle+Tier+Suite\n",
    "\n",
    " \n",
    "\n",
    "Do you happen to know if you know of how to get confluence to work from AWS Kendra or know of someone who was able to do that in the past?\n",
    "\n",
    " \n",
    "\n",
    "Thanks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's a summary of the email and the key points:\n",
      "\n",
      "Summary: The author is trying to access Confluence through Amazon Kendra but is having trouble. They have tried several methods and have even updated the cloud team's policies to ensure permissions are in place, but they are still unable to access the Confluence documentation. They are reaching out to the recipient to see if they know of a solution or have experience with this issue.\n",
      "\n",
      "Key points:\n",
      "\n",
      "* The author is trying to access Confluence through Amazon Kendra.\n",
      "* They have tried several methods and have updated the cloud team's policies.\n",
      "* They are unable to access the Confluence documentation.\n",
      "* They are reaching out to the recipient for help and advice.\n",
      "\n",
      "Sentiment: The author's tone is polite and professional, and they express a sense of frustration and helplessness. They are seeking assistance and guidance from the recipient.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize this email and extract the key points.\n",
    "What did the author say and what was the sentiment of the email?:\n",
    "\n",
    "email: {email2}\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt, model=\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing New Information in the Prompt\n",
    "\n",
    "A model's knowledge of the world ends at the moment of its training - so it won't know about more recent events.\n",
    "\n",
    "Llama 2 was released for research and commercial use on July 18, 2023, and its training ended some time before that date.\n",
    "\n",
    "Ask the model about an event, in this case, FIFA Women's World Cup 2023, which started on July 20, 2023, and see how the model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I'm just an AI, I don't have access to real-time or historical data on the number of British citizens killed during the Hamas attacks on Israel in 2023. The information you are seeking is likely to be sensitive and may not be publicly available due to privacy and security concerns.\n",
      "\n",
      "I would recommend consulting official sources, such as the UK government or the Israeli government, for information on the casualties of the conflict. These sources may provide information on the number of civilians, including British citizens, who were killed or injured during the conflict.\n",
      "\n",
      "It's important to note that the conflict between Hamas and Israel is a complex and sensitive issue, and it's essential to approach it with respect and sensitivity towards all parties involved. It's also important to rely on credible sources of information and to avoid spreading misinformation or propaganda.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "How many British citizens were killed during the Hamas attacks on Israel in 2023?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model still thinks that the tournament is yet to be played, even though you are now in 2024!\n",
    "\n",
    "Another thing to note is, July 18, 2023 was the date the model was released to public, and it was trained even before that, so it only has information upto that point. The response says, \"the final match is scheduled to take place in July 2023\", but the final match was played on August 20, 2023.\n",
    "\n",
    "You can provide the model with information about recent events, in this case text from Wikipedia about the 2023 Women's World Cup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "The 2023 FIFA Women's World Cup (MƒÅori: Ipu Wahine o te Ao FIFA i 2023)[1] was the ninth edition of the FIFA Women's World Cup, the quadrennial international women's football championship contested by women's national teams and organised by FIFA. The tournament, which took place from 20 July to 20 August 2023, was jointly hosted by Australia and New Zealand.[2][3][4] It was the first FIFA Women's World Cup with more than one host nation, as well as the first World Cup to be held across multiple confederations, as Australia is in the Asian confederation, while New Zealand is in the Oceanian confederation. It was also the first Women's World Cup to be held in the Southern Hemisphere.[5]\n",
    "This tournament was the first to feature an expanded format of 32 teams from the previous 24, replicating the format used for the men's World Cup from 1998 to 2022.[2] The opening match was won by co-host New Zealand, beating Norway at Eden Park in Auckland on 20 July 2023 and achieving their first Women's World Cup victory.[6]\n",
    "Spain were crowned champions after defeating reigning European champions England 1‚Äì0 in the final. It was the first time a European nation had won the Women's World Cup since 2007 and Spain's first title, although their victory was marred by the Rubiales affair.[7][8][9] Spain became the second nation to win both the women's and men's World Cup since Germany in the 2003 edition.[10] In addition, they became the first nation to concurrently hold the FIFA women's U-17, U-20, and senior World Cups.[11] Sweden would claim their fourth bronze medal at the Women's World Cup while co-host Australia achieved their best placing yet, finishing fourth.[12] Japanese player Hinata Miyazawa won the Golden Boot scoring five goals throughout the tournament. Spanish player Aitana Bonmat√≠ was voted the tournament's best player, winning the Golden Ball, whilst Bonmat√≠'s teammate Salma Paralluelo was awarded the Young Player Award. England goalkeeper Mary Earps won the Golden Glove, awarded to the best-performing goalkeeper of the tournament.\n",
    "Of the eight teams making their first appearance, Morocco were the only one to advance to the round of 16 (where they lost to France; coincidentally, the result of this fixture was similar to the men's World Cup in Qatar, where France defeated Morocco in the semi-final). The United States were the two-time defending champions,[13] but were eliminated in the round of 16 by Sweden, the first time the team had not made the semi-finals at the tournament, and the first time the defending champions failed to progress to the quarter-finals.[14]\n",
    "Australia's team, nicknamed the Matildas, performed better than expected, and the event saw many Australians unite to support them.[15][16][17] The Matildas, who beat France to make the semi-finals for the first time, saw record numbers of fans watching their games, their 3‚Äì1 loss to England becoming the most watched television broadcast in Australian history, with an average viewership of 7.13 million and a peak viewership of 11.15 million viewers.[18]\n",
    "It was the most attended edition of the competition ever held.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the information provided in the context, Spain won the 2023 Women's World Cup.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the following context, who won the 2023 Women's World cup?\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try asking questions of your own! Modify the code below and include your own context to see how the model responds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "Given the following context,\n",
      "How many British citizens were killed during the Hamas attacks on Israel in 2023?\n",
      "\n",
      "context: \n",
      "Prime Minister Rishi Sunak confirms that six British citizens were killed during the Hamas attacks on Israel, while a further ten are missing.[751]\n",
      "Two British teenage sisters, Noya and Yahel Sharabi, are among those missing, and believed to have been kidnapped, following the 7 October attacks on Israel. Their mother, Lianne, also a British citizen, was killed in the Be'eri massacre.[752][753] The following day the girls' family tells the BBC the Yahel was also murdered.[754] On 22 October the family release a statement to say Noya was also murdered.[755]\n",
      "Guardian cartoonist Steve Bell is sacked following a row over a drawing he created of Israeli Prime Minister Benjamin Netanyahu that was deemed to be antisemitic.[756]\n",
      "Justice Secretary Alex Chalk announces that prisons in England and Wales will be allowed to release some minor offenders on probation early in order to alleviate overcrowding.[757]\n",
      "\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/llama-2-7b-chat\n",
      "  According to the context, six British citizens were killed during the Hamas attacks on Israel in 2023.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Prime Minister Rishi Sunak confirms that six British citizens were killed during the Hamas attacks on Israel, while a further ten are missing.[751]\n",
    "Two British teenage sisters, Noya and Yahel Sharabi, are among those missing, and believed to have been kidnapped, following the 7 October attacks on Israel. Their mother, Lianne, also a British citizen, was killed in the Be'eri massacre.[752][753] The following day the girls' family tells the BBC the Yahel was also murdered.[754] On 22 October the family release a statement to say Noya was also murdered.[755]\n",
    "Guardian cartoonist Steve Bell is sacked following a row over a drawing he created of Israeli Prime Minister Benjamin Netanyahu that was deemed to be antisemitic.[756]\n",
    "Justice Secretary Alex Chalk announces that prisons in England and Wales will be allowed to release some minor offenders on probation early in order to alleviate overcrowding.[757]\n",
    "\"\"\"\n",
    "query = \"How many British citizens were killed during the Hamas attacks on Israel in 2023?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following context,\n",
    "{query}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                 verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-thought Prompting\n",
    "\n",
    "LLMs can perform better at reasoning and logic problems if you ask them to break the problem down into smaller steps. This is known as chain-of-thought prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, all 15 of you can get to the restaurant using the cars and motorcycles. Here's how:\n",
      "\n",
      "First, let's use the two cars, each seating 5 people. This will accommodate 10 people (5 + 5).\n",
      "\n",
      "Next, since there are only 15 - 10 = 5 people left, they can all ride on the two motorcycles, with 2 people per motorcycle.\n",
      "\n",
      "So, it is possible for all 15 people to get to the restaurant using the available cars and motorcycles.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\"\"\"\n",
    "response = llama(prompt, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the prompt to ask the model to \"think step by step\" about the math problem you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let's break this down:\n",
      "\n",
      "1. You have 15 people who want to go to a restaurant.\n",
      "2. There are two cars, each capable of seating 5 people. So, in total, the cars can accommodate 10 people (2 cars * 5 people per car).\n",
      "3. There are two motorcycles, each capable of fitting 2 people. So, in total, the motorcycles can carry 4 people (2 motorcycles * 2 people per motorcycle).\n",
      "4. Adding the capacity of the cars and motorcycles together, we get 14 seats (10 seats from cars + 4 seats from motorcycles).\n",
      "5. Since you only have 15 people, and there are 14 seats available, it is indeed possible for all of you to get to the restaurant using the cars and motorcycles.\n",
      "\n",
      "So, yes, you can all get to the restaurant by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "response = llama(prompt, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the model with additional instructions.\n",
    "\n",
    "This ^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1: Calculate the total number of people who can be transported by cars.\n",
      "Each car can seat 5 people, and there are 2 cars. So, the total number of people that can be transported by cars is 2 cars * 5 people/car = 10 people.\n",
      "\n",
      "Step 2: Calculate the total number of people who can be transported by motorcycles.\n",
      "Each motorcycle can fit 2 people, and there are 2 motorcycles. So, the total number of people that can be transported by motorcycles is 2 motorcycles * 2 people/motorcycle = 4 people.\n",
      "\n",
      "Step 3: Add the total number of people who can be transported by cars and motorcycles together.\n",
      "10 people (cars) + 4 people (motorcycles) = 14 people.\n",
      "\n",
      "Step 4: Compare the total number of people who can be transported by cars and motorcycles with the total number of people who need to go to the restaurant.\n",
      "In this case, 14 people is less than 15 people (the total number of people who want to go to the restaurant).\n",
      "\n",
      "Answer: No, we cannot all get to the restaurant by car or motorcycle because the transportation methods provided can only accommodate 14 people, and there are 15 people who want to go to the restaurant.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "Explain each intermediate step.\n",
    "Only when you are done with all your steps,\n",
    "provide the answer based on your intermediate steps.\n",
    "\"\"\"\n",
    "response = llama(prompt, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of instructions matters!\n",
    "\n",
    "Ask the model to \"answer first\" and \"explain later\" to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, we can all get to the restaurant by car or motorcycle.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. We have 15 people in total.\n",
      "2. Two cars can seat 5 people each, so together they can accommodate 10 people (5*2=10).\n",
      "3. Two motorcycles can fit 2 people each, so they can carry 4 people (2*2=4).\n",
      "4. Adding the seating capacity of cars and motorcycles, we get 14 (10 from cars + 4 from motorcycles).\n",
      "5. Since there are 15 people in total and the cars and motorcycles can carry 14, it seems like one person will have to go separately.\n",
      "6. However, the problem states that two people have motorcycles. These two people can ride their motorcycles to the restaurant, freeing up two spots in the cars for other people.\n",
      "7. With the two motorcycle riders now occupying car seats, the cars can now accommodate 10 people and the motorcycles can carry 2 people.\n",
      "8. In total, the cars and motorcycles can now carry 12 people (10 from cars + 2 from motorcycles), which is enough for everyone to get to the restaurant.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt, model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since LLMs predict their answer one token at a time, the best practice is to ask them to think step by step, and then only provide the answer after they have explained their reasoning.\n",
    "\n",
    "#### Keep prompting process iterative, add examples and instructions as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
