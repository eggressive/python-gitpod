{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import llama, code_llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing code to solve a math problem\n",
    "\n",
    "Lists of daily minimum and maximum temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_min = [42, 52, 47, 47, 53, 48, 47, 53, 55, 56, 57, 50, 48, 45]\n",
    "temp_max = [55, 57, 59, 59, 58, 62, 65, 65, 64, 63, 60, 60, 62, 62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the Llama 7B model to determine the day with the lowest temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the temperature forecast you provided, the day with the lowest temperature is Day 7, with a low temperature of 47°F (8.3°C).\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Below is the 14 day temperature forecast in fahrenheit degree:\n",
    "14-day low temperatures: {temp_min}\n",
    "14-day high temperatures: {temp_max}\n",
    "Which day has the lowest temperature?\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask Code Llama to write a python function to determine the minimum temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "[PYTHON]\n",
      "def get_min_max(temp_min, temp_max):\n",
      "    return min(temp_min), max(temp_max)\n",
      "[/PYTHON]\n",
      "[TESTS]\n",
      "# Test case 1:\n",
      "assert get_min_max([1, 2, 3], [4, 5, 6]) == (1, 6)\n",
      "# Test case 2:\n",
      "assert get_min_max([1, 2, 3], [4, 5, 6, 7]) == (1, 7)\n",
      "# Test case 3:\n",
      "assert get_min_max([1, 2, 3, 4], [4, 5, 6]) == (1, 6)\n",
      "[/TESTS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Write Python code that can calculate\n",
    "the minimum of the list temp_min\n",
    "and the maximum of the list temp_max\n",
    "\"\"\"\n",
    "response_2 = code_llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function on the temperature lists above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max(temp_min, temp_max):\n",
    "    return min(temp_min), max(temp_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 65)\n"
     ]
    }
   ],
   "source": [
    "temp_min = [42, 52, 47, 47, 53, 48, 47, 53, 55, 56, 57, 50, 48, 45]\n",
    "temp_max = [55, 57, 59, 59, 58, 62, 65, 65, 64, 63, 60, 60, 62, 62]\n",
    "\n",
    "results = get_min_max(temp_min, temp_max)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code in-filling\n",
    "\n",
    "Use Code Llama to fill in partially completed code.\n",
    "\n",
    "Notice the [INST] and [/INST] tags that have been added to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "def star_rating(n):\n",
      "'''\n",
      "  This function returns a rating given the number n,\n",
      "  where n is an integers from 1 to 5.\n",
      "'''\n",
      "\n",
      "    if n == 1:\n",
      "        rating=\"poor\"\n",
      "    <FILL>\n",
      "    elif n == 5:\n",
      "        rating=\"excellent\"\n",
      "\n",
      "    return rating\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/CodeLlama-7b-Instruct\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "def star_rating(n):\n",
    "'''\n",
    "  This function returns a rating given the number n,\n",
    "  where n is an integers from 1 to 5.\n",
    "'''\n",
    "\n",
    "    if n == 1:\n",
    "        rating=\"poor\"\n",
    "    <FILL>\n",
    "    elif n == 5:\n",
    "        rating=\"excellent\"\n",
    "\n",
    "    return rating\n",
    "\"\"\"\n",
    "\n",
    "response = code_llama(prompt,\n",
    "                      verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "[PYTHON]\n",
      "def star_rating(n):\n",
      "    if n == 1:\n",
      "        rating = \"poor\"\n",
      "    elif n == 2:\n",
      "        rating = \"fair\"\n",
      "    elif n == 3:\n",
      "        rating = \"average\"\n",
      "    elif n == 4:\n",
      "        rating = \"good\"\n",
      "    else:\n",
      "        rating = \"excellent\"\n",
      "    return rating\n",
      "[/PYTHON]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write code to calculate the nth Fibonacci number\n",
    "\n",
    "Here is the Fibonacci sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Code Llama to write a Fibonacci number\n",
    "\n",
    "- Write a natural language prompt that asks the model to write code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "Provide a function that calculates the n-th fibonacci number.\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/CodeLlama-7b-Instruct\n",
      "  \n",
      "[PYTHON]\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "[/PYTHON]\n",
      "[TESTS]\n",
      "# Test case 1:\n",
      "assert fibonacci(0) == 0\n",
      "# Test case 2:\n",
      "assert fibonacci(1) == 1\n",
      "# Test case 3:\n",
      "assert fibonacci(2) == 1\n",
      "# Test case 4:\n",
      "assert fibonacci(3) == 2\n",
      "# Test case 5:\n",
      "assert fibonacci(6) == 8\n",
      "[/TESTS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a function that calculates the n-th fibonacci number.\n",
    "\"\"\"\n",
    "\n",
    "response = code_llama(prompt, verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the code more efficient\n",
    "\n",
    "Ask Code Llama to critique its initial response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "For the following code: \n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "Is this implementation efficient?\n",
      "Please explain.\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/CodeLlama-7b-Instruct\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "prompt_1 = f\"\"\"\n",
    "For the following code: {code}\n",
    "Is this implementation efficient?\n",
    "Please explain.\n",
    "\"\"\"\n",
    "response_1 = code_llama(prompt_1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No, this implementation is not efficient. The time complexity of this function is O(2^n), which means that the time it takes to compute the nth Fibonacci number grows exponentially with the size of the input.\n",
      "\n",
      "The reason for this is that the function calls itself recursively twice, once for each term in the Fibonacci sequence. Each recursive call requires the computation of the previous two terms, which means that the time complexity grows exponentially with the size of the input.\n",
      "\n",
      "A more efficient implementation of the Fibonacci sequence would be to use a loop instead of recursion, like this:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    a, b = 0, 1\n",
      "    for i in range(n):\n",
      "        a, b = b, a + b\n",
      "    return a\n",
      "```\n",
      "This implementation has a time complexity of O(n), which means that the time it takes to compute the nth Fibonacci number grows linearly with the size of the input.\n",
      "\n",
      "In general, it's important to be aware of the time complexity of your algorithms and to choose implementations that have a lower time complexity. This can help you avoid running into performance issues when dealing with large inputs.\n"
     ]
    }
   ],
   "source": [
    "print(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the original and more efficient implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_fast(n):\n",
    "    a, b = 0, 1\n",
    "    for i in range(n):\n",
    "        a, b = b, a + b\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the runtimes of the two functions\n",
    "\n",
    "Start by asking Code Llama to write Python code that calculates how long a piece of code takes to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "Provide sample code that calculates the runtime of a Python function call.\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/CodeLlama-7b-Instruct\n",
      "  Here is an example of how you can calculate the runtime of a Python function call using the `time` module:\n",
      "```\n",
      "import time\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "start_time = time.time()\n",
      "my_function()\n",
      "end_time = time.time()\n",
      "\n",
      "print(\"Runtime:\", end_time - start_time)\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds.\n",
      "\n",
      "Alternatively, you can use the `timeit` module to measure the runtime of a function call. Here is an example:\n",
      "```\n",
      "import timeit\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "print(timeit.timeit(my_function))\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds.\n",
      "\n",
      "You can also use the `cProfile` module to profile the runtime of a function call. Here is an example:\n",
      "```\n",
      "import cProfile\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "cProfile.run('my_function()')\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds, as well as other information such as the number of calls and the total time spent in the function.\n",
      "\n",
      "You can also use the `profile` module to profile the runtime of a function call. Here is an example:\n",
      "```\n",
      "import profile\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "profile.run('my_function()')\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds, as well as other information such as the number of calls and the total time spent in the function.\n",
      "\n",
      "You can also use the `line_profiler` module to profile the runtime of a function call. Here is an example:\n",
      "```\n",
      "import line_profiler\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "line_profiler.run('my_function()')\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds, as well as other information such as the number of calls and the total time spent in the function.\n",
      "\n",
      "You can also use the `memory_profiler` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import memory_profiler\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "memory_profiler.run('my_function()')\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `psutil` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import psutil\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "psutil.memory_usage(my_function)\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `pympler` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import pympler\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "pympler.memory_usage(my_function)\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `memory_profiler` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import memory_profiler\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "memory_profiler.run('my_function()')\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `psutil` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import psutil\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "psutil.memory_usage(my_function)\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `pympler` module to profile the memory usage of a\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Provide sample code that calculates the runtime \\\n",
    "of a Python function call.\n",
    "\"\"\"\n",
    "\n",
    "response = code_llama(prompt, verbose=True)\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the original Fibonacci code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recursive fibonacci(40) \n",
      "runtime in seconds: 14.68470287322998\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n=40\n",
    "start_time = time.time()\n",
    "fibonacci(n) # note, we recommend keeping this number <=40\n",
    "end_time = time.time()\n",
    "print(f\"recursive fibonacci({n}) \")\n",
    "print(f\"runtime in seconds: {end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the efficient implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-recursive fibonacci(40) \n",
      "runtime in seconds: 6.413459777832031e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n=40\n",
    "start_time = time.time()\n",
    "fibonacci_fast(n) # note, we recommend keeping this number <=40\n",
    "end_time = time.time()\n",
    "print(f\"non-recursive fibonacci({n}) \")\n",
    "print(f\"runtime in seconds: {end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Llama can take in longer text\n",
    "\n",
    "Code Llama models can handle much larger input text than the Llama Chat models - more than 20,000 characters.\n",
    "\n",
    "The size of the input text is known as the `context window`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response from Llama 2 7B Chat model\n",
    "\n",
    "The following code will return an error because the sum of the input and output tokens is larger than the limit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4097. Given: 3975 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/TheVelveteenRabbit.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "prompt=f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n \n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Ask the 7B model to respond\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response from Code Llama 7B Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The text is a story about a velveteen rabbit who is loved by a boy. The story explores the theme of realness and how it is defined by the boy's love for the rabbit. The story also touches on the idea of shabbiness and how it is not a bad thing for a toy to become real. The story is written in a simple and straightforward style, making it easy to follow and understand. The use of descriptive language and vivid imagery helps to create a sense of wonder and magic in the story. The story also has a happy ending, which is fitting for a children's tale. Overall, the story is a delightful and enjoyable read for children and adults alike.\n"
     ]
    }
   ],
   "source": [
    "from modules.utils import llama\n",
    "with open(\"data/TheVelveteenRabbit.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "prompt=f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n \n",
    "{text}\n",
    "\"\"\"\n",
    "response = code_llama(prompt)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
