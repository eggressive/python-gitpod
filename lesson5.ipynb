{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Llama Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import llama, llama_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Sentiment Classification\n",
    "\n",
    "Compare the models on few-shot prompt sentiment classification.\n",
    "\n",
    "You are asking the model to return a one word response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: Positive\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "Message: Can't wait to order pizza for dinner tonight!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use the 7B parameter chat model (llama-2-7b-chat) to get the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hungry\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 model=\"togethercomputer/llama-2-7b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the 70B parameter chat model (llama-2-70b-chat) on the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Positive\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt,\n",
    "                 model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Summarization\n",
    "\n",
    "Compare the models on summarization task.\n",
    "\n",
    "This is the same \"email\" as the one you used previously in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Amit,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task â€” the input and the desired output â€” sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you're unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn't work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn't deliver the performance you want, then try fine-tuning â€” but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? ðŸ˜œ)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that's not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM's output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning â€” in which GPT-4 surpasses current open models â€” it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, it's also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I'll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "\n",
    "What did the author say about llama models?\n",
    "```\n",
    "{email}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use the 7B parameter chat model (llama-2-7b-chat) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The author discusses the proliferation of large language models (LLMs) and the various ways to build applications using them, ranging from prompting to fine-tuning. Key points include:\n",
      "\n",
      "1. LLMs are becoming more open source, giving developers more options for building applications.\n",
      "2. Different approaches to building applications include prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\n",
      "3. The author recommends starting with prompting for most teams, as it allows for quick development, and gradually moving to more complex techniques if needed.\n",
      "4. Fine-tuning a smaller model can yield superior results than prompting a larger, more capable model in some cases, depending on the application.\n",
      "5. Choosing a specific model also requires consideration, as smaller models require less processing power but may not have as much knowledge about the world or reasoning ability as larger models.\n",
      "6. The author plans to discuss how to make this choice in a future letter.\n",
      "\n",
      "Regarding llama models, the author mentions that a member of the DeepLearning.AI team has been trying to fine-tune a model called Llama-2-7B to sound like them, but the author does not provide any further information or insights about llama models.\n"
     ]
    }
   ],
   "source": [
    "response_7b = llama(prompt,\n",
    "                model=\"togethercomputer/llama-2-7b-chat\")\n",
    "print(response_7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the 13B parameter chat model (llama-2-13b-chat) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's a summary of the email and some key points:\n",
      "\n",
      "Summary:\n",
      "The author discusses different approaches to building applications using large language models (LLMs), ranging from prompting to fine-tuning. They recommend starting with prompting and gradually increasing the complexity of the techniques based on the desired output quality. The author also discusses the choice of model size and the trade-offs between smaller models and larger, more capable models.\n",
      "\n",
      "Key points:\n",
      "\n",
      "1. Prompting is a quick and easy way to build applications using LLMs, and it can be done with minimal resources.\n",
      "2. One-shot or few-shot prompting can yield better results than just giving a pretrained LLM instructions.\n",
      "3. Fine-tuning can deliver high-quality outputs, but it requires more resources and a larger dataset.\n",
      "4. Pretraining an LLM from scratch is a complex and resource-intensive task, but it can lead to specialized models with a high level of knowledge.\n",
      "5. The choice of development approach and model size depends on the specific application and the desired level of complexity and performance.\n",
      "6. Fine-tuning a smaller model can be effective for changing the style of an LLM's output, but it may not yield superior results for complex reasoning tasks.\n",
      "7. The author recommends starting with prompting and gradually increasing the complexity of the techniques based on the desired output quality.\n"
     ]
    }
   ],
   "source": [
    "response_13b = llama(prompt,\n",
    "                model=\"togethercomputer/llama-2-13b-chat\")\n",
    "print(response_13b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, use the 70B parameter chat model (llama-2-70b-chat) to summarize the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The author of the email discusses the various ways to build applications using large language models (LLMs), including prompting, one-shot or few-shot prompting, fine-tuning, and pretraining. The author recommends starting with prompting and gradually moving on to more complex techniques if necessary. They also mention that fine-tuning a smaller model may not always yield superior results compared to prompting a larger, more capable model, and that the choice of model depends on the application.\n",
      "\n",
      "The author also mentions that there are now many open-source LLMs available, which gives developers more options for building applications. They also mention that there are courses available to learn more about these options and how to use them effectively.\n",
      "\n",
      "In a humorous aside, the author mentions that a member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like the author, and wonders if their job is at risk.\n",
      "\n",
      "Some key points from the email include:\n",
      "\n",
      "* There are several ways to build applications using LLMs, including prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\n",
      "* The choice of development approach depends on the complexity and resources available for the project.\n",
      "* The choice of model also depends on the application, with smaller models requiring less processing power but larger models having more knowledge and better reasoning ability.\n",
      "* There are now many open-source LLMs available, which gives developers more options for building applications.\n",
      "* There are courses available to learn more about how to use LLMs effectively.\n"
     ]
    }
   ],
   "source": [
    "response_70b = llama(prompt,\n",
    "                model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response_70b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Graded Evaluation: Summarization\n",
    "\n",
    "Interestingly, you can ask a LLM to evaluate the responses of other LLMs.\n",
    "\n",
    "This is known as `Model-Graded Evaluation`.\n",
    "\n",
    "Create a prompt that will evaluate these three responses using 70B parameter chat model (llama-2-70b-chat).\n",
    "\n",
    "In the prompt, provide the \"email\", \"name of the models\", and the \"summary\" generated by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the summaries provided, it seems that all three models (Llama-2-7B-chat, Llama-2-13B-chat, and Llama-2-70B-chat) were able to accurately summarize the main points of the email. However, there are some differences in the level of detail and clarity provided by each model.\n",
      "\n",
      "Llama-2-7B-chat's summary is the shortest and most concise of the three, focusing on the main points of the email such as the different approaches to building applications using LLMs and the trade-offs between model size and complexity. However, it does not provide as much detail or examples as the other two models.\n",
      "\n",
      "Llama-2-13B-chat's summary is slightly longer and provides more examples and details about the different approaches to building applications using LLMs. It also highlights the author's recommendation to start with prompting and gradually increase the complexity of the techniques based on the desired output quality. However, it does not provide as much information about the choice of model size and the trade-offs between smaller and larger models as the other two models.\n",
      "\n",
      "Llama-2-70B-chat's summary is the longest and most detailed of the three, providing a comprehensive overview of the email's main points and key takeaways. It covers all the different approaches to building applications using LLMs, including prompting, one-shot or few-shot prompting, fine-tuning, and pretraining. It also discusses the choice of model size and the trade-offs between smaller models and larger, more capable models. Additionally, it mentions the author's recommendation to start with prompting and gradually increase the complexity of the techniques based on the desired output quality.\n",
      "\n",
      "Overall, all three models were able to accurately summarize the main points of the email, but Llama-2-70B-chat's summary is the most comprehensive and detailed. Based on this evaluation, I would recommend using Llama-2-70B-chat for future summarization tasks that require a high level of detail and clarity.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the original text denoted by `email`\n",
    "and the name of several models: `model:<name of model>\n",
    "as well as the summary generated by that model: `summary`\n",
    "\n",
    "Provide an evaluation of each model's summary:\n",
    "- Does it summarize the original text well?\n",
    "- Does it follow the instructions of the prompt?\n",
    "- Are there any other interesting characteristics of the model's output?\n",
    "\n",
    "Then compare the models based on their evaluation \\\n",
    "and recommend the models that perform the best.\n",
    "\n",
    "email: ```{email}`\n",
    "\n",
    "model: llama-2-7b-chat\n",
    "summary: {response_7b}\n",
    "\n",
    "model: llama-2-13b-chat\n",
    "summary: {response_13b}\n",
    "\n",
    "model: llama-2-70b-chat\n",
    "summary: {response_70b}\n",
    "\"\"\"\n",
    "\n",
    "response_eval = llama(prompt,\n",
    "                model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
